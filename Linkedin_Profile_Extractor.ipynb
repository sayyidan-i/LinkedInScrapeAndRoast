{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linkedin Scrapping Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Library and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: webdriver_manager in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: urllib3~=1.26 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: requests in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from webdriver_manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sayyidan\\appdata\\roaming\\python\\python39\\site-packages (from webdriver_manager) (24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sayyidan\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sayyidan\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (3.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\sayyidan\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\sayyidan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver_manager beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the webdriver and login information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "username = config['LINKEDIN']['username']\n",
    "password = config['LINKEDIN']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "driver.find_element(By.ID, \"username\").send_keys(username)\n",
    "driver.find_element(By.ID, \"password\").send_keys(password)\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.url_contains(\"feed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll down to load all the posts\n",
    "\n",
    "def scroll_down(n, pixels):\n",
    "    # Scroll down 500 pixels up to 30 times or until no more scrolling is possible\n",
    "    for _ in range(n):\n",
    "        driver.execute_script(f\"window.scrollBy(0, {pixels});\")\n",
    "        time.sleep(3)  # Wait for 3 seconds for new content to load\n",
    "\n",
    "        # Check if we can scroll further\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height <= driver.execute_script(\"return window.scrollY + window.innerHeight\"):\n",
    "            break  # Break if no more scrolling is possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the profile headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PROFILE = \"https://www.linkedin.com/in/sayyidan-i/\"\n",
    "driver.get(URL_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Sayyidan Muhamad Ikhsan\n",
      "Headline: Accelerator Startup Program Intern at Indigo by Telkom | Machine Learning Enthusiast\n",
      "Connections: 221\n"
     ]
    }
   ],
   "source": [
    "# Profile Headline\n",
    "name_element = driver.find_element(By.XPATH, \"//h1[@class='text-heading-xlarge inline t-24 v-align-middle break-words']\")\n",
    "headline_element = driver.find_element(By.XPATH, \"//div[@class='text-body-medium break-words']\")\n",
    "connections_element = driver.find_element(By.XPATH, \"//span[@class='t-bold']\")\n",
    "\n",
    "\n",
    "# Extract and print the connection number\n",
    "name = name_element.text.strip()\n",
    "headline = headline_element.text.strip()\n",
    "connections = connections_element.text.strip()\n",
    "\n",
    "print(f\"Name: {name}\")\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Connections: {connections}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Sayyidan Muhamad Ikhsan, a passionate and versatile final-year student at Universitas Gadjah Mada, blending a mosaic of experiences in various fields of organizational leadership to entrepreneurial ventures and community service. Driven by an unwavering passion for artificial intelligence, I continue to learn and adapt seamlessly to dynamic challenges. My journey has shaped a versatile professional characterized by adaptability, a strong work ethic, and insatiable curiosity. With a high sense of responsibility, I consistently strive for excellence - in both independent and team efforts. I am ready to channel my adaptability, passion for continuous learning, and deep interest in artificial intelligence into meaningful contributions across various roles and projects.\n"
     ]
    }
   ],
   "source": [
    "# Get the page source\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Initialize a variable to store the skills data\n",
    "about = \"\"\n",
    "\n",
    "# Find the div that contains the skills information\n",
    "about_div = soup.find('div', class_='SXjmahSTOQiuxpKMNmYxiEpFsQMjwfVyogEk inline-show-more-text--is-collapsed inline-show-more-text--is-collapsed-with-line-clamp full-width')\n",
    "\n",
    "if about_div:\n",
    "    # Extract text from the visually-hidden span\n",
    "    hidden_span = about_div.find('span', class_='visually-hidden')\n",
    "    if hidden_span:\n",
    "        about = hidden_span.get_text(strip=True)\n",
    "    else:\n",
    "        about = \"Skills not found\"\n",
    "else:\n",
    "    about = \"Skills div not found\"\n",
    "\n",
    "# Print the extracted skills data\n",
    "print(about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the extracted profile information\n",
    "profile_data = {\n",
    "    'Name': [name],\n",
    "    'Headline': [headline],\n",
    "    'Connections': [connections],\n",
    "    'About': [about]\n",
    "}\n",
    "\n",
    "df_profile = pd.DataFrame(profile_data)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df_profile.to_csv('linkedin_profile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the LinkedIn profile activity page\n",
    "ACTIVITY_URL = f'{URL_PROFILE}detail/recent-activity/'\n",
    "driver.get(ACTIVITY_URL)\n",
    "\n",
    "# Scroll down 500 pixels up to 30 times or until no more scrolling is possible\n",
    "scroll_down(30, 500)\n",
    "\n",
    "# Get the page source after scrolling\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Prepare a list to collect post data\n",
    "posts_data = []\n",
    "\n",
    "# Find all posts\n",
    "posts = soup.find_all(\"div\", class_=\"display-flex flex-column flex-grow-1\")\n",
    "\n",
    "# Collect the required information from each post\n",
    "for post in posts:\n",
    "    # Initialize a dictionary to hold post data\n",
    "    post_info = {}\n",
    "\n",
    "    # Check for repost status\n",
    "    repost_check = post.find(\"span\", class_=\"update-components-header__text-view\")\n",
    "    post_info['Is Reposted'] = repost_check and \"reposted this\" in repost_check.get_text(strip=True)\n",
    "\n",
    "    # Extract time posted from visually hidden span\n",
    "    time_posted = post.find(\"span\", class_=\"update-components-actor__sub-description t-12 t-normal t-black--light\")\n",
    "    hidden_time = time_posted.find(\"span\", class_=\"visually-hidden\")\n",
    "    post_info['Time Posted'] = hidden_time.get_text(strip=True) if hidden_time else None\n",
    "\n",
    "    # Extract post caption\n",
    "    post_caption = post.find(\"div\", class_=\"update-components-text relative update-components-update-v2__commentary\")\n",
    "    post_info['Post Caption'] = post_caption.get_text(strip=True) if post_caption else None\n",
    "\n",
    "    # Extract reaction count\n",
    "    reaction_span = post.find(\"span\", class_=\"social-details-social-counts__reactions-count\")\n",
    "    reaction_count = reaction_span.get_text(strip=True) if reaction_span else '0'\n",
    "    post_info['Reaction Count'] = int(reaction_count.replace(',', ''))  # Remove commas before converting\n",
    "\n",
    "    # Extract comment count\n",
    "    comment_button = post.find(\"button\", aria_label=True)\n",
    "    comment_count = comment_button['aria-label'].split()[0] if comment_button else '0'\n",
    "    post_info['Comment Count'] = int(comment_count.replace(',', ''))  # Remove commas before converting\n",
    "\n",
    "    # Append the post information to the list\n",
    "    posts_data.append(post_info)\n",
    "\n",
    "if not posts_data:\n",
    "    print(\"No posts found on the activity page\")\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(posts_data)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('linkedin_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Position  \\\n",
      "0                 Accelerator Startup Program Intern   \n",
      "1                            Machine Learning Cohort   \n",
      "2               Electromedicine Assistant Internship   \n",
      "3                Staff of Kementrian Ekonomi Kreatif   \n",
      "4              Publication at Pekan Wirausaha Teknik   \n",
      "5                                   Staff of Adkesma   \n",
      "6  Staff of Consumption, Logistics, and Transport...   \n",
      "7                               Freelance Math Tutor   \n",
      "8                           Staff of Public Relation   \n",
      "9           Video Production Division of Teknik Fair   \n",
      "\n",
      "                                              Status  \\\n",
      "0                         Indigo Telkom · Internship   \n",
      "1  Bangkit Academy led by Google, Tokopedia, Goje...   \n",
      "2  Medika Plaza (PT Kartika Bina Medikatama) · In...   \n",
      "3  Medika Plaza (PT Kartika Bina Medikatama) · In...   \n",
      "4  Medika Plaza (PT Kartika Bina Medikatama) · In...   \n",
      "5                                      KMTETI FT UGM   \n",
      "6                                          NESCO UGM   \n",
      "7                              Gauthmath · Freelance   \n",
      "8                                      Forkomasi UGM   \n",
      "9  Student Executive Board Faculty of Engineering...   \n",
      "\n",
      "                               Time  \\\n",
      "0        Apr 2024 - Present · 6 mos   \n",
      "1       Aug 2023 - Feb 2024 · 7 mos   \n",
      "2       Jan 2023 - Feb 2023 · 2 mos   \n",
      "3       Feb 2022 - Jul 2022 · 6 mos   \n",
      "4       Feb 2022 - Jul 2022 · 6 mos   \n",
      "5       Jan 2022 - Jul 2022 · 7 mos   \n",
      "6  Nov 2020 - Jun 2022 · 1 yr 8 mos   \n",
      "7       Jan 2022 - Mar 2022 · 3 mos   \n",
      "8       Oct 2021 - Feb 2022 · 5 mos   \n",
      "9       Jul 2021 - Nov 2021 · 5 mos   \n",
      "\n",
      "                                             Caption  \n",
      "0  As an Accelerator Startup Program Intern, I he...  \n",
      "1  As a distinguished graduate of Bangkit Academy...  \n",
      "2                                  Caption not found  \n",
      "3                                  Caption not found  \n",
      "4                                  Caption not found  \n",
      "5                                  Caption not found  \n",
      "6                                  Caption not found  \n",
      "7                                  Caption not found  \n",
      "8                                  Caption not found  \n",
      "9                                  Caption not found  \n"
     ]
    }
   ],
   "source": [
    "#open the LinkedIn profile experience page\n",
    "URL_EXP = f'{URL_PROFILE}details/experience/'\n",
    "\n",
    "# Your previous code to load the page\n",
    "driver.get(URL_EXP)\n",
    "time.sleep(5)  # Allow the page to load\n",
    "scroll_down(30, 500)\n",
    "\n",
    "# Get the page source after loading\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find all experience containers\n",
    "experience_list = soup.find_all('li', class_='pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column')\n",
    "\n",
    "# Initialize a list to store the extracted data\n",
    "experiences = []\n",
    "\n",
    "# Iterate over each experience container\n",
    "for experience in experience_list:\n",
    "    # Try to find the position div with class \"mr1 t-bold\"\n",
    "    position_div = experience.find('div', class_='display-flex align-items-center mr1 t-bold')\n",
    "\n",
    "    if position_div:\n",
    "        # Extract the single position from the visually-hidden span\n",
    "        position_span = position_div.find('span', class_='visually-hidden')\n",
    "        if position_span:\n",
    "            position = position_span.get_text(strip=True)\n",
    "        \n",
    "        # Get status\n",
    "        status_div = experience.find('span', class_='t-14 t-normal')\n",
    "        if status_div:\n",
    "            status_span = status_div.find('span', class_='visually-hidden')\n",
    "            status = status_span.get_text(strip=True) if status_span else \"Status not found\"\n",
    "        \n",
    "        # Get time information\n",
    "        time_div = experience.find('span', class_='pvs-entity__caption-wrapper')\n",
    "        if time_div:\n",
    "            time_info = time_div.get_text(strip=True)\n",
    "        else:\n",
    "            time_info = \"Time not found\"\n",
    "        \n",
    "        # Get caption data from visually-hidden span\n",
    "        caption_div = experience.find('div', class_='display-flex align-items-center t-14 t-normal t-black')\n",
    "        if caption_div:\n",
    "            caption_span = caption_div.find('span', class_='visually-hidden')\n",
    "            caption = caption_span.get_text(strip=True) if caption_span else \"Caption not found\"\n",
    "            # Clean the caption by replacing new lines with dots\n",
    "            caption = caption.replace('\\n', '. ').replace('\\r', '')  # Replace new lines with dots\n",
    "        else:\n",
    "            caption = \"Caption not found\"\n",
    "        \n",
    "        # Add the data to the list without location\n",
    "        experiences.append({\n",
    "            'Position': position,\n",
    "            'Status': status,\n",
    "            'Time': time_info,\n",
    "            'Caption': caption\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        # Handle the case where there are multiple positions under one location\n",
    "        multiple_positions_divs = experience.find_all('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold')\n",
    "        \n",
    "        # Extract the relevant status from the custom class for multiple positions\n",
    "        location_status_div = experience.find('div', class_='fNWbxLGnqELwvLQkpFOBqndmvYRoRbCrA GZxrfCTVzkMgqhWuZbUiEolViXjfRzCMeTUvwKA')\n",
    "        if location_status_div:\n",
    "            # Extract the visually-hidden span text for status\n",
    "            status_div = location_status_div.find('span', class_='t-14 t-normal')\n",
    "            if status_div:\n",
    "                status_span = status_div.find('span', class_='visually-hidden')\n",
    "                status = status_span.get_text(strip=True) if status_span else \"Status not found\"\n",
    "        \n",
    "            # Get time information\n",
    "            time_div = location_status_div.find('span', class_='pvs-entity__caption-wrapper')\n",
    "            if time_div:\n",
    "                time_info = time_div.get_text(strip=True)\n",
    "            else:\n",
    "                time_info = \"Time not found\"\n",
    "        \n",
    "        # Skip the first div (location), and create separate rows for each position\n",
    "        for index, position_div in enumerate(multiple_positions_divs):\n",
    "            if index == 0:\n",
    "                continue  # Skip the first entry (location)\n",
    "            \n",
    "            # Extract the visually-hidden span text\n",
    "            position_span = position_div.find('span', class_='visually-hidden')\n",
    "            if position_span:\n",
    "                position = position_span.get_text(strip=True)\n",
    "                \n",
    "                # Add the data to the list without location\n",
    "                experiences.append({\n",
    "                    'Position': position,\n",
    "                    'Status': status,\n",
    "                    'Time': time_info,\n",
    "                    'Caption': caption\n",
    "                })\n",
    "\n",
    "\n",
    "if not experiences:\n",
    "    experiences.append({'Position': 'Experiences not found', 'Status': 'Status not found', 'Time': 'Time not found', 'Caption': 'Caption not found'})\n",
    "\n",
    "# Create a DataFrame from the experiences list\n",
    "df = pd.DataFrame(experiences)\n",
    "print(df)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('experience_details.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      University  \\\n",
      "0  Universitas Gadjah Mada (UGM)   \n",
      "\n",
      "                                               Field     Description  \n",
      "0  Bachelor of Engineering - BE, biomedical engin...  Skills:English  \n"
     ]
    }
   ],
   "source": [
    "# open volunteer page\n",
    "URL_education = f'{URL_PROFILE}details/education/'\n",
    "driver.get(URL_education)\n",
    "time.sleep(5)  # Allow the page to load\n",
    "scroll_down(30, 500)\n",
    "\n",
    "# Get the page source after loading\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Initialize an empty list for education details\n",
    "education_details = []\n",
    "\n",
    "# Find the education container\n",
    "education_container = soup.find('div', class_='scaffold-finite-scroll__content')\n",
    "\n",
    "# Extract each education entry\n",
    "if education_container:\n",
    "    education_entries = education_container.find_all('li', class_='pvs-list__paged-list-item')  # Adjust if necessary\n",
    "\n",
    "    for entry in education_entries:\n",
    "        # Get university information\n",
    "        university_div = entry.find('div', class_='display-flex align-items-center mr1 hoverable-link-text t-bold')\n",
    "        university = university_div.find('span', class_='visually-hidden').get_text(strip=True) if university_div else \"University not found\"\n",
    "        \n",
    "        # Get field of study\n",
    "        field_div = entry.find('span', class_='t-14 t-normal')\n",
    "        field = field_div.find('span', class_='visually-hidden').get_text(strip=True) if field_div else \"Field not found\"\n",
    "        \n",
    "        # Get description\n",
    "        description_div = entry.find('div', class_='display-flex align-items-center t-14 t-normal t-black')\n",
    "        description = description_div.find('span', class_='visually-hidden').get_text(strip=True) if description_div else \"Description not found\"\n",
    "        \n",
    "        # Append to the education details list\n",
    "        education_details.append({\n",
    "            'University': university,\n",
    "            'Field': field,\n",
    "            'Description': description\n",
    "        })\n",
    "\n",
    "# If no education details were found, add a default entry\n",
    "if not education_details:\n",
    "    education_details.append({'University': 'Education not found', 'Field': 'Field not found', 'Description': 'Description not found'})\n",
    "\n",
    "# Create a DataFrame from the education details list\n",
    "df = pd.DataFrame(education_details)\n",
    "print(df)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('education_details.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Project Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name            date            caption\n",
      "0  Projects not found  Date not found  Caption not found\n"
     ]
    }
   ],
   "source": [
    "# Open the LinkedIn profile projects page\n",
    "PROJECTS_URL = f'{URL_PROFILE}details/projects/'\n",
    "driver.get(PROJECTS_URL)\n",
    "time.sleep(5)  # Allow the page to load\n",
    "scroll_down(30, 500)\n",
    "\n",
    "# Get the page source after loading\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find the outer div that contains all projects\n",
    "projects_div = soup.find('div', class_='scaffold-finite-scroll scaffold-finite-scroll--infinite')\n",
    "#print(projects_div)\n",
    "\n",
    "# Find all project name divs within that outer div\n",
    "if projects_div:\n",
    "    project_divs = projects_div.find_all('div', class_='display-flex align-items-center mr1 t-bold')\n",
    "    # Initialize a list to store project details\n",
    "    \n",
    "    \n",
    "    projects = []\n",
    "\n",
    "    for project_div in project_divs:\n",
    "        # Extract the project name\n",
    "        project_name = project_div.get_text(strip=True)\n",
    "\n",
    "        # Find the time span related to this project\n",
    "        time_span = project_div.find_next('span', class_='t-14 t-normal')\n",
    "        if time_span is not None:\n",
    "            date_info = time_span.find('span', class_='visually-hidden')\n",
    "            if date_info:\n",
    "                time_data = date_info.get_text(strip=True)\n",
    "            else:\n",
    "                time_data = \"Date not found\"\n",
    "        else:\n",
    "            time_data = \"Time span not found\"\n",
    "\n",
    "        # Find the caption for this project\n",
    "        caption_div = project_div.find_next('li', class_='pvs-list__item--with-top-padding huOBYMIdZizXtwNCBcwmBCRytomUXHoLNnetHMA')\n",
    "        if caption_div:\n",
    "            caption_info = caption_div.find('span', class_='visually-hidden')\n",
    "            if caption_info:\n",
    "                caption_data = caption_info.get_text(strip=True)\n",
    "                caption_data = caption_data.replace('\\n', '. ')\n",
    "            else:\n",
    "                caption_data = \"Caption not found\"\n",
    "        else:\n",
    "            caption_data = \"Caption div not found\"\n",
    "\n",
    "        # Append the project details to the projects list\n",
    "        projects.append({\n",
    "            'name': project_name,\n",
    "            'date': time_data,\n",
    "            'caption': caption_data\n",
    "        })\n",
    "\n",
    "if not projects:\n",
    "    projects.append({'name': 'Projects not found', 'date': 'Date not found', 'caption': 'Caption not found'})\n",
    "\n",
    "# Create a DataFrame from the projects list\n",
    "df = pd.DataFrame(projects)\n",
    "print(df)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('projects_details.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Volunteer Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       name  \\\n",
      "0      Data Collector IGDData Collector IGD   \n",
      "1      Education CampaignEducation Campaign   \n",
      "2  Mental Health RangerMental Health Ranger   \n",
      "\n",
      "                                   date            caption  \n",
      "0                           Indorelawan  Caption not found  \n",
      "1           Character Matters Indonesia  Caption not found  \n",
      "2  Satu Persen - Indonesian Life School  Caption not found  \n"
     ]
    }
   ],
   "source": [
    "# open volunteer page\n",
    "URL_VOLUNTEER = f'{URL_PROFILE}details/volunteering-experiences/'\n",
    "driver.get(URL_VOLUNTEER)\n",
    "time.sleep(5)  # Allow the page to load\n",
    "scroll_down(30, 500)\n",
    "\n",
    "# Get the page source after loading\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find the outer div that contains all projects\n",
    "volunteer_div = soup.find('div', class_='scaffold-finite-scroll scaffold-finite-scroll--infinite')\n",
    "#print(volunteer_div)\n",
    "\n",
    "volunteer = []\n",
    "# Find all project name divs within that outer div\n",
    "if volunteer_div:\n",
    "    volunteer_divs = volunteer_div.find_all('div', class_='display-flex align-items-center mr1 t-bold')\n",
    "    # Initialize a list to store project details\n",
    "\n",
    "    for volunteer_div in volunteer_divs:\n",
    "        # Extract the project name\n",
    "        volunteer_name = volunteer_div.get_text(strip=True)\n",
    "\n",
    "        # Find the time span related to this project\n",
    "        time_span = volunteer_div.find_next('span', class_='t-14 t-normal')\n",
    "        if time_span is not None:\n",
    "            date_info = time_span.find('span', class_='visually-hidden')\n",
    "            if date_info:\n",
    "                time_data = date_info.get_text(strip=True)\n",
    "            else:\n",
    "                time_data = \"Date not found\"\n",
    "        else:\n",
    "            time_data = \"Time span not found\"\n",
    "\n",
    "        # Find the caption for this project\n",
    "        caption_div = volunteer_div.find_next('li', class_='pvs-list__item--with-top-padding huOBYMIdZizXtwNCBcwmBCRytomUXHoLNnetHMA')\n",
    "        if caption_div:\n",
    "            caption_info = caption_div.find('span', class_='visually-hidden')\n",
    "            if caption_info:\n",
    "                caption_data = caption_info.get_text(strip=True)\n",
    "                caption_data = caption_data.replace('\\n', '. ')\n",
    "            else:\n",
    "                caption_data = \"Caption not found\"\n",
    "        else:\n",
    "            caption_data = \"Caption div not found\"\n",
    "\n",
    "        # Append the project details to the volunteer list\n",
    "        volunteer.append({\n",
    "            'name': volunteer_name,\n",
    "            'date': time_data,\n",
    "            'caption': caption_data\n",
    "        })\n",
    "\n",
    "if not volunteer:\n",
    "    volunteer.append({'name': 'volunteer not found', 'date': 'Date not found', 'caption': 'Caption not found'})\n",
    "\n",
    "# Create a DataFrame from the volunteer list\n",
    "df = pd.DataFrame(volunteer)\n",
    "print(df)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('volunteer_details.csv', index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get License and Certification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 name          organization  \\\n",
      "0   Advanced Learning AlgorithmsAdvanced Learning ...       DeepLearning.AI   \n",
      "1   Introduction to TensorFlow for Artificial Inte...       DeepLearning.AI   \n",
      "2   Supervised Machine Learning: Regression and Cl...       DeepLearning.AI   \n",
      "3   Unsupervised Learning, Recommenders, Reinforce...       DeepLearning.AI   \n",
      "4   Analyze Data to Answer QuestionsAnalyze Data t...                Google   \n",
      "5   Ask Questions to Make Data-Driven DecisionsAsk...                Google   \n",
      "6   Calculus for Machine Learning and Data Science...       DeepLearning.AI   \n",
      "7   Foundations: Data, Data, EverywhereFoundations...                Google   \n",
      "8   Linear Algebra for Machine Learning and Data S...       DeepLearning.AI   \n",
      "9   Prepare Data for ExplorationPrepare Data for E...                Google   \n",
      "10  Probability & Statistics for Machine Learning ...       DeepLearning.AI   \n",
      "11  Process Data from Dirty to CleanProcess Data f...                Google   \n",
      "12  Share Data Through the Art of VisualizationSha...                Goggle   \n",
      "13  Share Data Through the Art of VisualizationSha...                Google   \n",
      "14       Crash Course on PythonCrash Course on Python                Google   \n",
      "15  Introduction to Git and GitHubIntroduction to ...                Google   \n",
      "16  Using Python to Interact with the Operating Sy...                Google   \n",
      "17  Excel Fundamentals for Data AnalysisExcel Fund...  Macquarie University   \n",
      "18         Data Wrangling PythonData Wrangling Python                 DQLab   \n",
      "19  Fundamental SQL Using SELECT StatementFundamen...                 DQLab   \n",
      "20  Python Fundamental for Data SciencePython Fund...                 DQLab   \n",
      "21  R Fundamental For Data ScienceR Fundamental Fo...                 DQLab   \n",
      "22  Statistics using R for Data ScienceStatistics ...                 DQLab   \n",
      "23  Revou Mini Course - Introduction to Digital Ma...                 RevoU   \n",
      "\n",
      "        issued_date  \n",
      "0   Issued Oct 2023  \n",
      "1   Issued Oct 2023  \n",
      "2   Issued Oct 2023  \n",
      "3   Issued Oct 2023  \n",
      "4   Issued Sep 2023  \n",
      "5   Issued Sep 2023  \n",
      "6   Issued Sep 2023  \n",
      "7   Issued Sep 2023  \n",
      "8   Issued Sep 2023  \n",
      "9   Issued Sep 2023  \n",
      "10  Issued Sep 2023  \n",
      "11  Issued Sep 2023  \n",
      "12  Issued Sep 2023  \n",
      "13  Issued Sep 2023  \n",
      "14  Issued Aug 2023  \n",
      "15  Issued Aug 2023  \n",
      "16  Issued Aug 2023  \n",
      "17  Issued Jul 2022  \n",
      "18  Issued Nov 2021  \n",
      "19  Issued Nov 2021  \n",
      "20  Issued Nov 2021  \n",
      "21  Issued Nov 2021  \n",
      "22  Issued Nov 2021  \n",
      "23  Issued Feb 2021  \n"
     ]
    }
   ],
   "source": [
    "# open volunteer page\n",
    "URL_LICENSES = f'{URL_PROFILE}details/certifications/'\n",
    "driver.get(URL_LICENSES)\n",
    "time.sleep(5)  # Allow the page to load\n",
    "scroll_down(30, 500)\n",
    "\n",
    "\n",
    "# Get the page source after loading\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use Beautiful Soup to parse the page\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find the outer div that contains all projects\n",
    "licenses_div = soup.find('div', class_='scaffold-finite-scroll scaffold-finite-scroll--infinite')\n",
    "#print(volunteer_div)\n",
    "\n",
    "licenses = []\n",
    "# Find all project name divs within that outer div\n",
    "if licenses_div:\n",
    "    licenses_divs = licenses_div.find_all('div', class_='display-flex flex-wrap align-items-center full-height')\n",
    "    # Initialize a list to store project details\n",
    "\n",
    "    for licenses_div in licenses_divs:\n",
    "        # Extract the project name\n",
    "        licenses_name = licenses_div.get_text(strip=True)\n",
    "\n",
    "        # Find the time span related to this project\n",
    "        organization = licenses_div.find_next('span', class_='t-14 t-normal')\n",
    "        if organization:\n",
    "            organization_info = organization.find('span', class_='visually-hidden')\n",
    "            if organization_info:\n",
    "                organization_data = organization_info.get_text(strip=True)\n",
    "            else:\n",
    "                organization_data = \"Organization not found\"\n",
    "            \n",
    "        # Find the issued date for every license\n",
    "        issued_date = licenses_div.find_next('span', class_='pvs-entity__caption-wrapper')\n",
    "        if issued_date:\n",
    "            issued_date_info = issued_date.get_text(strip=True)\n",
    "        else:\n",
    "            issued_date_info = \"Issued date not found\"\n",
    "\n",
    "        # Append the project details to the licenses list\n",
    "        licenses.append({\n",
    "            'name': licenses_name,\n",
    "            'organization': organization_data,\n",
    "            'issued_date': issued_date_info\n",
    "        })\n",
    "\n",
    "if not licenses:\n",
    "    licenses.append({'name': 'licenses not found', 'organization': 'Date not found'})\n",
    "\n",
    "# Create a DataFrame from the licenses list\n",
    "df = pd.DataFrame(licenses)\n",
    "print(df)\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('csv_data\\licenses_details.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
